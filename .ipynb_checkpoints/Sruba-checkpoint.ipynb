{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7d4be76a-5941-41eb-b20c-f4896269a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seeds\n",
    "rnd.seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6f47a64d-2a86-4cac-8980-50ac9dd39ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of question pairs:  345036\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  id  qid1  qid2                                          question1  \\\n",
       "0      0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1      1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2      2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3      3   6    13    14                                Should I buy tiago?   \n",
       "4      4   7    15    16                     How can I be a good geologist?   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  What keeps childern active and far from phone ...             0  \n",
       "4          What should I do to be a great geologist?             1  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"quora_duplicate_questions.csv\")\n",
    "N = len(data)\n",
    "print('Number of question pairs: ', N)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6dd52479-5098-4c9a-8240-59e38f315fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 300000 Test set: 45036\n"
     ]
    }
   ],
   "source": [
    "N_train = 300000\n",
    "N_test = 45036\n",
    "data_train = data[:N_train]\n",
    "data_test = data[N_train:N_train + N_test]\n",
    "print(\"Train set:\", len(data_train), \"Test set:\", len(data_test))\n",
    "del (data)  # remove to free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "af2de9d7-da6e-4971-a43c-7e403c8ad2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate questions:  119221\n",
      "Indexes of first ten duplicate questions: [4, 8, 9, 10, 11, 12, 14, 16, 25, 27]\n"
     ]
    }
   ],
   "source": [
    "td_index = data_train['is_duplicate'] == 1\n",
    "td_index = [i for i, x in enumerate(td_index) if x]\n",
    "print('Number of duplicate questions: ', len(td_index))\n",
    "print('Indexes of first ten duplicate questions:', td_index[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6c760595-b210-47d8-80b7-f7488dec9b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I be a good geologist?\n",
      "What should I do to be a great geologist?\n",
      "is_duplicate:  1\n"
     ]
    }
   ],
   "source": [
    "print(data_train['question1'][4])\n",
    "print(data_train['question2'][4])\n",
    "print('is_duplicate: ', data_train['is_duplicate'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "946852bc-8b39-4e17-9908-8267c997f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_train = np.array(data_train['question1'][td_index])\n",
    "Q2_train = np.array(data_train['question2'][td_index])\n",
    "\n",
    "Q1_test = np.array(data_test['question1'])\n",
    "Q2_test = np.array(data_test['question2'])\n",
    "y_test  = np.array(data_test['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0cd61335-c36e-4c4f-958b-fbd2ac97adc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING QUESTIONS:\n",
      "\n",
      "Question 1:  How can I be a good geologist?\n",
      "Question 2:  What should I do to be a great geologist? \n",
      "\n",
      "Question 1:  What would a Trump presidency mean for current international masterâ€™s students on an F1 visa?\n",
      "Question 2:  How will a Trump presidency affect the students presently in US or planning to study in US? \n",
      "\n",
      "TESTING QUESTIONS:\n",
      "\n",
      "Question 1:  How can I move photos in Google Photos into folders or albums?\n",
      "Question 2:  Can we make a hidden folder in Google photos? \n",
      "\n",
      "is_duplicate = 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('TRAINING QUESTIONS:\\n')\n",
    "print('Question 1: ', Q1_train[0])\n",
    "print('Question 2: ', Q2_train[0], '\\n')\n",
    "print('Question 1: ', Q1_train[4])\n",
    "print('Question 2: ', Q2_train[4], '\\n')\n",
    "\n",
    "print('TESTING QUESTIONS:\\n')\n",
    "print('Question 1: ', Q1_test[0])\n",
    "print('Question 2: ', Q2_test[0], '\\n')\n",
    "print('is_duplicate =', y_test[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "07aef896-c8b9-4d69-afdd-dc719252a894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate questions:  119221\n",
      "The length of the training set is:   95376\n",
      "The length of the validation set is:  23845\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "cut_off = int(len(Q1_train) * 0.8)\n",
    "train_Q1, train_Q2 = Q1_train[:cut_off], Q2_train[:cut_off]\n",
    "val_Q1, val_Q2 = Q1_train[cut_off:], Q2_train[cut_off:]\n",
    "print('Number of duplicate questions: ', len(Q1_train))\n",
    "print(\"The length of the training set is:  \", len(train_Q1))\n",
    "print(\"The length of the validation set is: \", len(val_Q1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "109fc73e-fd73-4e3a-972f-fc63c704dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "text_vectorization = tf.keras.layers.TextVectorization(output_mode='int',split='whitespace', standardize='strip_punctuation')\n",
    "text_vectorization.adapt(np.concatenate((Q1_train,Q2_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "501bc66d-8501-4cec-ae62-b04eb1a6ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: Siamese\n",
    "def Siamese(text_vectorizer, vocab_size=36224, d_feature=128):\n",
    "    \"\"\"Returns a Siamese model.\n",
    "\n",
    "    Args:\n",
    "        text_vectorizer (TextVectorization): TextVectorization instance, already adapted to your training data.\n",
    "        vocab_size (int, optional): Length of the vocabulary. Defaults to 56400.\n",
    "        d_model (int, optional): Depth of the model. Defaults to 128.\n",
    "        \n",
    "    Returns:\n",
    "        tf.model.Model: A Siamese model. \n",
    "    \n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Define the Siamese branch\n",
    "    branch = tf.keras.models.Sequential(name='sequential') \n",
    "    # Add the text_vectorizer layer. This is the text_vectorizer you instantiated and trained before \n",
    "    branch.add(text_vectorizer)\n",
    "    # Add the Embedding layer. Remember to call it 'embedding' using the parameter `name`\n",
    "    branch.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=d_feature, name='embedding'))\n",
    "    # Add the LSTM layer, recall from W2 that you want the LSTM layer to return sequences, not just one value. \n",
    "    # Remember to call it 'LSTM' using the parameter `name`\n",
    "    branch.add(tf.keras.layers.LSTM(units=d_feature, name='LSTM', return_sequences=True))\n",
    "    # Add the GlobalAveragePooling1D layer. Remember to call it 'mean' using the parameter `name`\n",
    "    branch.add(tf.keras.layers.GlobalAveragePooling1D(name='mean'))\n",
    "    # Add the normalizing layer using the Lambda function. Remember to call it 'out' using the parameter `name`\n",
    "    branch.add(tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1), name='out'))\n",
    "    \n",
    "    # Define both inputs. Remember to call them 'input_1' and 'input_2' using the `name` parameter. \n",
    "    # Be mindful of the data type and size\n",
    "    input1 = tf.keras.layers.Input(shape=(1,), dtype=tf.string, name='input_1')\n",
    "    input2 = tf.keras.layers.Input(shape=(1,), dtype=tf.string, name='input_2')\n",
    "    \n",
    "    # Define the output of each branch of your Siamese network. Remember that both branches have the same coefficients, \n",
    "    # but they each receive different inputs.\n",
    "    branch1 = branch(input1)\n",
    "    branch2 = branch(input2)\n",
    "    \n",
    "    # Define the Concatenate layer. You should concatenate columns, you can fix this using the `axis` parameter. \n",
    "    # This layer is applied over the outputs of each branch of the Siamese network\n",
    "    conc = tf.keras.layers.Concatenate(axis=1, name='conc_1_2')([branch1, branch2]) \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return tf.keras.models.Model(inputs=[input1, input2], outputs=conc, name=\"SiameseModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "035f8513-097c-4d22-a6ef-ae8b1c86d1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseModel\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " sequential (Sequential)     (None, 128)                  4688896   ['input_1[0][0]',             \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " conc_1_2 (Concatenate)      (None, 256)                  0         ['sequential[0][0]',          \n",
      "                                                                     'sequential[1][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4688896 (17.89 MB)\n",
      "Trainable params: 4688896 (17.89 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (Text  (None, None)              0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 128)         4557312   \n",
      "                                                                 \n",
      " LSTM (LSTM)                 (None, None, 128)         131584    \n",
      "                                                                 \n",
      " mean (GlobalAveragePooling  (None, 128)               0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " out (Lambda)                (None, 128)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4688896 (17.89 MB)\n",
      "Trainable params: 4688896 (17.89 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Check your Siamese model\n",
    "model = Siamese(text_vectorization, vocab_size=text_vectorization.vocabulary_size())\n",
    "model.build(input_shape=((None, 1), (None, 1)))  # Set the input_shape argument\n",
    "model.summary()\n",
    "\n",
    "# Check the sequential branch inside the Siamese model\n",
    "sequential_branch = model.get_layer(name='sequential')\n",
    "sequential_branch.build(input_shape=(None,))  # Set the input_shape argument\n",
    "sequential_branch.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3fffdb08-a3b1-48bf-82cb-9b0b0146de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: TripletLossFn\n",
    "def TripletLossFn(v1, v2, margin=0.25):\n",
    "    \"\"\"Custom Loss function.\n",
    "\n",
    "    Args:\n",
    "        v1 (numpy.ndarray or Tensor): Array with dimension (batch_size, model_dimension) associated with Q1.\n",
    "        v2 (numpy.ndarray or Tensor): Array with dimension (batch_size, model_dimension) associated with Q2.\n",
    "        margin (float, optional): Desired margin. Defaults to 0.25.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss (numpy.ndarray or Tensor)\n",
    "    \"\"\"\n",
    "\n",
    "    # use `tf.linalg.matmul` to take the dot product of the two batches.\n",
    "    # Don't forget to transpose the second argument using `transpose_b=True`\n",
    "    scores = tf.linalg.matmul(v2, tf.transpose(v1, perm=[1, 0]))\n",
    "\n",
    "    # calculate new batch size and cast it as the same datatype as scores.\n",
    "    batch_size = tf.cast(tf.shape(v1)[0], scores.dtype)\n",
    "\n",
    "    # use `tf.linalg.diag_part` to grab the cosine similarity of all positive examples\n",
    "    positive = tf.linalg.diag_part(scores)\n",
    "\n",
    "    # subtract the diagonal from scores. You can do this by creating a diagonal matrix with the values\n",
    "    # of all positive examples using `tf.linalg.diag`\n",
    "    negative_zero_on_duplicate = scores - tf.linalg.diag(positive)\n",
    "\n",
    "    # use `tf.math.reduce_sum` on `negative_zero_on_duplicate` for `axis=1` and divide it by `(batch_size - 1)`\n",
    "    mean_negative = tf.math.reduce_sum(negative_zero_on_duplicate, axis=1) / (batch_size - 1)\n",
    "\n",
    "    # create a composition of two masks:\n",
    "    # the first mask to extract the diagonal elements,\n",
    "    # the second mask to extract elements in the negative_zero_on_duplicate matrix that are larger than the elements in the diagonal\n",
    "    mask_exclude_positives = tf.math.logical_or(tf.eye(batch_size, dtype=tf.bool), (negative_zero_on_duplicate > tf.expand_dims(positive, 1)))\n",
    "\n",
    "    # multiply `mask_exclude_positives` with 2.0 and subtract it out of `negative_zero_on_duplicate`\n",
    "    negative_without_positive = tf.where(mask_exclude_positives, -2.0, negative_zero_on_duplicate)\n",
    "\n",
    "    # take the row by row `max` of `negative_without_positive`.\n",
    "    # Hint: `tf.math.reduce_max(negative_without_positive, axis=None)`\n",
    "    closest_negative = tf.math.reduce_max(negative_without_positive, axis=1)\n",
    "\n",
    "    # compute `tf.maximum` among 0.0 and `A`\n",
    "    # A = subtract `positive` from `margin` and add `closest_negative`\n",
    "    triplet_loss1 = tf.maximum(0.0, margin - positive + closest_negative)\n",
    "\n",
    "    # compute `tf.maximum` among 0.0 and `B`\n",
    "    # B = subtract `positive` from `margin` and add `mean_negative`\n",
    "    triplet_loss2 = tf.maximum(0.0, margin - positive + mean_negative)\n",
    "\n",
    "    # add the two losses together and take the `tf.math.reduce_sum` of it\n",
    "    triplet_loss = tf.math.reduce_sum(triplet_loss1 + triplet_loss2)\n",
    "\n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "117e4475-6a44-48e0-a9f5-2e7f4faff216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TripletLoss(labels, out, margin=0.25):\n",
    "    _, embedding_size = out.shape # get embedding size\n",
    "    v1 = out[:,:int(embedding_size/2)] # Extract v1 from out\n",
    "    v2 = out[:,int(embedding_size/2):] # Extract v2 from out\n",
    "    return TripletLossFn(v1, v2, margin=margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e7f6a7a0-8d38-48aa-aacb-3e1550a14fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(((train_Q1, train_Q2),tf.constant([1]*len(train_Q1))))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(((val_Q1, val_Q2),tf.constant([1]*len(val_Q1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b750d617-a414-44e9-badd-43c29dc43af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: train_model\n",
    "def train_model(Siamese, TripletLoss, text_vectorizer, train_dataset, val_dataset, d_feature=128, lr=0.01, train_steps=5):\n",
    "    \"\"\"Training the Siamese Model\n",
    "\n",
    "    Args:\n",
    "        Siamese (function): Function that returns the Siamese model.\n",
    "        TripletLoss (function): Function that defines the TripletLoss loss function.\n",
    "        text_vectorizer: trained instance of `TextVecotrization` \n",
    "        train_dataset (tf.data.Dataset): Training dataset\n",
    "        val_dataset (tf.data.Dataset): Validation dataset\n",
    "        d_feature (int, optional) = size of the encoding. Defaults to 128.\n",
    "        lr (float, optional): learning rate for optimizer. Defaults to 0.01\n",
    "        train_steps (int): number of epochs\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model\n",
    "    \"\"\"\n",
    "    ## START CODE HERE ###\n",
    "\n",
    "    # Instantiate your Siamese model\n",
    "    model = Siamese(text_vectorizer,\n",
    "                    vocab_size = text_vectorizer.vocabulary_size(), #set vocab_size accordingly to the size of your vocabulary\n",
    "                    d_feature = d_feature)\n",
    "    # Compile the model\n",
    "    model.compile(loss=TripletLoss,\n",
    "                  optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "            )\n",
    "    # Train the model \n",
    "    model.fit(train_dataset,\n",
    "              epochs = train_steps,\n",
    "              validation_data = val_dataset,\n",
    "             )\n",
    "             \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca228653-2d05-43e0-a823-fc56be400b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "373/373 [==============================] - 89s 227ms/step - loss: 29.0934 - val_loss: 11.9404\n",
      "Epoch 2/25\n",
      "373/373 [==============================] - 82s 220ms/step - loss: 8.2248 - val_loss: 9.0588\n",
      "Epoch 3/25\n",
      "373/373 [==============================] - 85s 228ms/step - loss: 5.9085 - val_loss: 8.1458\n",
      "Epoch 4/25\n",
      "373/373 [==============================] - 83s 221ms/step - loss: 5.1913 - val_loss: 8.0184\n",
      "Epoch 5/25\n",
      "373/373 [==============================] - 86s 230ms/step - loss: 4.8901 - val_loss: 7.6087\n",
      "Epoch 6/25\n",
      "373/373 [==============================] - 87s 234ms/step - loss: 4.6051 - val_loss: 7.4576\n",
      "Epoch 7/25\n",
      "373/373 [==============================] - 83s 220ms/step - loss: 4.4843 - val_loss: 7.5680\n",
      "Epoch 8/25\n",
      "373/373 [==============================] - 83s 222ms/step - loss: 4.3608 - val_loss: 7.4660\n",
      "Epoch 9/25\n",
      "373/373 [==============================] - 82s 219ms/step - loss: 4.2946 - val_loss: 7.4549\n",
      "Epoch 10/25\n",
      "373/373 [==============================] - 79s 210ms/step - loss: 4.2899 - val_loss: 7.3748\n",
      "Epoch 11/25\n",
      " 89/373 [======>.......................] - ETA: 56s - loss: 3.9256"
     ]
    }
   ],
   "source": [
    "train_steps = 25\n",
    "batch_size = 256\n",
    "train_generator = train_dataset.shuffle(len(train_Q1),\n",
    "                                        seed=7, \n",
    "                                        reshuffle_each_iteration=True).batch(batch_size=batch_size)\n",
    "val_generator = val_dataset.shuffle(len(val_Q1), \n",
    "                                   seed=7,\n",
    "                                   reshuffle_each_iteration=True).batch(batch_size=batch_size)\n",
    "model = train_model(Siamese, TripletLoss,text_vectorization, \n",
    "                                            train_generator, \n",
    "                                            val_generator, \n",
    "                                            train_steps=train_steps,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0424c2c-1cd6-471f-83bf-5ca3907b79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: classify\n",
    "def classify(test_Q1, test_Q2, y_test, threshold, model, batch_size=64, verbose=True):\n",
    "    \"\"\"Function to test the accuracy of the model.\n",
    "\n",
    "    Args:\n",
    "        test_Q1 (numpy.ndarray): Array of Q1 questions. Each element of the array would be a string.\n",
    "        test_Q2 (numpy.ndarray): Array of Q2 questions. Each element of the array would be a string.\n",
    "        y_test (numpy.ndarray): Array of actual target.\n",
    "        threshold (float): Desired threshold\n",
    "        model (tensorflow.Keras.Model): The Siamese model.\n",
    "        batch_size (int, optional): Size of the batches. Defaults to 64.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model\n",
    "        numpy.array: confusion matrix\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    test_gen = tf.data.Dataset.from_tensor_slices(((test_Q1, test_Q2),None)).batch(batch_size=batch_size)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    pred = model.predict(test_gen)\n",
    "    _, n_feat = pred.shape\n",
    "    v1 = pred[:, :n_feat // 2]\n",
    "    v2 = pred[:, n_feat // 2:]\n",
    "    \n",
    "    # Compute the cosine similarity. Using `tf.math.reduce_sum`. \n",
    "    # Don't forget to use the appropriate axis argument.\n",
    "    d  = tf.math.reduce_sum(v1 * v2, axis=1) / (tf.norm(v1, axis=1) * tf.norm(v2, axis=1))\n",
    "    # Check if d>threshold to make predictions\n",
    "    y_pred = tf.cast(d > threshold, tf.float64)\n",
    "    # take the average of correct predictions to get the accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, y_test), tf.float64))\n",
    "    # compute the confusion matrix using `tf.math.confusion_matrix`\n",
    "    cm = tf.math.confusion_matrix(y_test, y_pred, num_classes=2)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return accuracy, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f053039e-dbbe-4bbb-9451-87397c1aeaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Ensure that all elements are strings\n",
    "Q1_test = [str(item) for item in Q1_test]\n",
    "Q2_test = [str(item) for item in Q2_test]\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(Q1_test + Q2_test)\n",
    "\n",
    "# Convert text to sequences\n",
    "Q1_sequences = tokenizer.texts_to_sequences(Q1_test)\n",
    "Q2_sequences = tokenizer.texts_to_sequences(Q2_test)\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_sequence_length = 100  # adjust as needed\n",
    "Q1_padded = pad_sequences(Q1_sequences, maxlen=max_sequence_length)\n",
    "Q2_padded = pad_sequences(Q2_sequences, maxlen=max_sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2473117b-d6a2-4956-bd10-ed3b5c0ac6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes around 1 minute\n",
    "accuracy, cm = classify(Q1_test,Q2_test, y_test, 0.7, model,  batch_size = 512) \n",
    "print(\"Accuracy\", accuracy.numpy())\n",
    "print(f\"Confusion matrix:\\n{cm.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651ef761-8b33-4d95-9f25-c70e6e260209",
   "metadata": {},
   "source": [
    "# UPTO ABOVE COURESERA CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "496c5e8f-d66b-48a6-b367-5c8eece8b34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sruba Sarkar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\Sruba Sarkar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "901/901 [==============================] - 6s 5ms/step - loss: 0.6143 - accuracy: 0.6695 - val_loss: 0.5856 - val_accuracy: 0.6979\n",
      "Epoch 2/10\n",
      "901/901 [==============================] - 4s 4ms/step - loss: 0.5664 - accuracy: 0.7054 - val_loss: 0.5622 - val_accuracy: 0.7039\n",
      "Epoch 3/10\n",
      "901/901 [==============================] - 4s 4ms/step - loss: 0.5302 - accuracy: 0.7271 - val_loss: 0.5546 - val_accuracy: 0.7137\n",
      "Epoch 4/10\n",
      "901/901 [==============================] - 4s 4ms/step - loss: 0.5002 - accuracy: 0.7490 - val_loss: 0.5593 - val_accuracy: 0.7115\n",
      "Epoch 5/10\n",
      "901/901 [==============================] - 4s 5ms/step - loss: 0.4693 - accuracy: 0.7704 - val_loss: 0.5554 - val_accuracy: 0.7119\n",
      "Epoch 6/10\n",
      "901/901 [==============================] - 4s 4ms/step - loss: 0.4406 - accuracy: 0.7855 - val_loss: 0.5586 - val_accuracy: 0.7130\n",
      "Epoch 7/10\n",
      "901/901 [==============================] - 4s 5ms/step - loss: 0.4119 - accuracy: 0.8028 - val_loss: 0.5930 - val_accuracy: 0.7136\n",
      "Epoch 8/10\n",
      "901/901 [==============================] - 4s 4ms/step - loss: 0.3793 - accuracy: 0.8223 - val_loss: 0.6166 - val_accuracy: 0.7043\n",
      "Epoch 9/10\n",
      "901/901 [==============================] - 4s 4ms/step - loss: 0.3515 - accuracy: 0.8367 - val_loss: 0.6468 - val_accuracy: 0.7168\n",
      "Epoch 10/10\n",
      "901/901 [==============================] - 4s 4ms/step - loss: 0.3219 - accuracy: 0.8545 - val_loss: 0.6994 - val_accuracy: 0.7176\n",
      "282/282 [==============================] - 6s 3ms/step\n",
      "Accuracy:  0.7253552397868561\n",
      "Confusion Matrix: \n",
      " [[4696  894]\n",
      " [1580 1838]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "google_news_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Convert text to word embeddings\n",
    "def text_to_embedding(text, embedding_dim):\n",
    "    tokens = text.split(' ')\n",
    "    vectors = [google_news_model[word] for word in tokens if word in google_news_model.key_to_index]\n",
    "    vector_mean = np.mean(vectors, axis=0) if vectors else np.zeros(embedding_dim)\n",
    "    return vector_mean\n",
    "\n",
    "# Apply the function to the questions\n",
    "embedding_dim = 300\n",
    "X = np.array([text_to_embedding(Q1_test[i], embedding_dim) for i in range(len(Q1_test))])\n",
    "Y = np.array([text_to_embedding(Q2_test[i], embedding_dim) for i in range(len(Q2_test))])\n",
    "X = np.concatenate((X, Y), axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_test, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=embedding_dim*2, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.predict(X_test)\n",
    "predictions = [1 if p > 0.5 else 0 for p in predictions]\n",
    "\n",
    "# Print the accuracy and confusion matrix\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b6112-f319-4984-957e-7c32d668d0cd",
   "metadata": {},
   "source": [
    "# Janina ki but normal GoogleNewsVector apply kora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ca336c-61e3-4a00-a1c5-ee5e49a079eb",
   "metadata": {},
   "source": [
    "# ar ei nicher ta i g Siamese with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5fc075db-1e37-4d21-bcdf-8151c70bebf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1408/1408 [==============================] - 65s 45ms/step\n",
      "1408/1408 [==============================] - 63s 44ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def text_to_embedding(text, embedding_dim):\n",
    "    tokens = text.split(' ')\n",
    "    vectors = [google_news_model[word] for word in tokens if word in google_news_model.key_to_index]\n",
    "    vector_mean = np.mean(vectors, axis=0) if vectors else np.zeros(embedding_dim)\n",
    "    return vector_mean\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "google_news_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# Set parameters\n",
    "max_len = 100\n",
    "embedding_dim = 300\n",
    "\n",
    "# Convert texts to sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(Q1_test)\n",
    "Q1_seq = tokenizer.texts_to_sequences(Q1_test)\n",
    "Q1_seq = pad_sequences(Q1_seq, maxlen=max_len)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(Q2_test)\n",
    "Q2_seq = tokenizer.texts_to_sequences(Q2_test)\n",
    "Q2_seq = pad_sequences(Q2_seq, maxlen=max_len)\n",
    "\n",
    "# Create a Siamese network with LSTM\n",
    "def create_siamese_lstm():\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    embedding = Embedding(input_dim=45036, \n",
    "                          output_dim=google_news_model.vector_size,\n",
    "                          trainable=False)(inputs)\n",
    "    lstm = LSTM(64, return_sequences=True)(embedding)\n",
    "    lstm = Dropout(0.2)(lstm)\n",
    "    lstm = LSTM(32)(lstm)\n",
    "    lstm = Dropout(0.2)(lstm)\n",
    "    lstm = Dense(64, activation='relu')(lstm)\n",
    "    model = Model(inputs, lstm)\n",
    "    return model\n",
    "    \n",
    "# Compile and train the Siamese network\n",
    "model = create_siamese_lstm()\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Compute embeddings for questions\n",
    "X = model.predict(Q1_seq)\n",
    "Y = model.predict(Q2_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "745e3846-0884-4e19-8fda-9916350ecb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_embeddings(X, Y, y_test, threshold, verbose=True):\n",
    "    # Compute cosine similarity scores between X and Y\n",
    "    similarity_scores = [cosine_similarity([x], [y])[0, 0] for x, y in zip(X, Y)]\n",
    "    \n",
    "    # Convert similarity scores to binary predictions using the threshold\n",
    "    y_pred = np.array(similarity_scores) > threshold\n",
    "\n",
    "    # Compute accuracy and confusion matrix\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    confusion_matrix = np.array([[np.sum((y_test == 0) & (y_pred == 0)), np.sum((y_test == 0) & (y_pred == 1))],\n",
    "                                 [np.sum((y_test == 1) & (y_pred == 0)), np.sum((y_test == 1) & (y_pred == 1))]])\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix)\n",
    "\n",
    "    return accuracy, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a9a6be15-0fd8-4e77-ae83-acf0ef402faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1_seq shape: (45036, 100)\n",
      "Q2_seq shape: (45036, 100)\n",
      "X shape: (45036, 64)\n",
      "Y shape: (45036, 64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Q1_seq shape:\", Q1_seq.shape)\n",
    "print(\"Q2_seq shape:\", Q2_seq.shape)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Y shape:\", Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "09fb9c2c-3c6d-4f40-9d2e-bc6cddb43f50",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (45036,) (9008,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Use the function to compute accuracy\u001b[39;00m\n\u001b[0;32m      5\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# You can adjust the threshold as needed\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m accuracy, cm \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_with_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfusion Matrix:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[60], line 9\u001b[0m, in \u001b[0;36mclassify_with_embeddings\u001b[1;34m(X, Y, y_test, threshold, verbose)\u001b[0m\n\u001b[0;32m      6\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(similarity_scores) \u001b[38;5;241m>\u001b[39m threshold\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Compute accuracy and confusion matrix\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43my_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m)\n\u001b[0;32m     10\u001b[0m confusion_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[np\u001b[38;5;241m.\u001b[39msum((y_test \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (y_pred \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)), np\u001b[38;5;241m.\u001b[39msum((y_test \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (y_pred \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m))],\n\u001b[0;32m     11\u001b[0m                              [np\u001b[38;5;241m.\u001b[39msum((y_test \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m (y_pred \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)), np\u001b[38;5;241m.\u001b[39msum((y_test \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m (y_pred \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m))]])\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (45036,) (9008,) "
     ]
    }
   ],
   "source": [
    "# Reshape y_test to match the shape of y_pred\n",
    "y_test = np.reshape(y_test, (-1,))\n",
    "\n",
    "# Use the function to compute accuracy\n",
    "threshold = 0.5  # You can adjust the threshold as needed\n",
    "accuracy, cm = classify_with_embeddings(X, Y, y_test, threshold)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb49b8cb-8d97-48bf-bfed-63ca8e7c090a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
